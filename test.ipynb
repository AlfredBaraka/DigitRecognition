{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](asset/shanaly_Ai.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all library that will used \n",
    "import torch\n",
    "import matplotlib\n",
    "import os\n",
    "import sklearn\n",
    "import sys\n",
    "import PIL\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win32\n",
      "torchvision version 0.20.1+cpu\n",
      "Pillow version 11.0.0\n",
      "torch version 2.5.1+cpu \n",
      "matplotlib version 3.9.2\n",
      "Sklearn version 1.5.2\n"
     ]
    }
   ],
   "source": [
    "# Print out the version of library \n",
    "print(sys.platform)\n",
    "print(f\"torchvision version {torchvision.__version__}\")\n",
    "print(f\"Pillow version {PIL.__version__}\")\n",
    "print(f\"torch version {torch.__version__} \")\n",
    "print(f\"matplotlib version {matplotlib.__version__}\")\n",
    "print(f\"Sklearn version {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all important class and module used \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.datasets import load_digits\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.out = nn.Linear(32*2*2, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        flatten = nn.Flatten()\n",
    "        x = flatten(x)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = CNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1797, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "digits = load_digits()\n",
    "images = digits.images\n",
    "target = digits.target\n",
    "tensor_images = torch.from_numpy(images).float()\n",
    "print(tensor_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Prepare Dataset\n",
    "class Dataset(Data.Dataset):\n",
    "    def __init__(self, images, target):\n",
    "        self.images = images\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        target = self.target[index]\n",
    "        image = image.unsqueeze(0)  \n",
    "        return (image, target)\n",
    "\n",
    "dataset = Dataset(tensor_images, target)\n",
    "train_loader = Data.DataLoader(dataset=dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print(dataset[1][0].shape) # the reason we squueze is to made a 3D with [channel, length, width ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "          [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "          [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "          [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "          [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "          [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "          [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "          [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]]]),\n",
       " np.int64(0))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0] # example of one data contain image and its target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 50\n",
    "Lr = 0.001\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=Lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, dataloader):\n",
    "    # Training the model\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for img, target in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "            optimizer.zero_grad()  \n",
    "            output = model(img)\n",
    "            loss = criterion(output, target)  \n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Loss = {total_loss/len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575ba1d3cc9a4f4891bd9b7e4d1ea5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.001117730934734733\n"
     ]
    }
   ],
   "source": [
    "train(1, model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, device=\"cpu\"):\n",
    "    all_probs = torch.tensor([]).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(data_loader, desc=\"Predicting\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            all_probs = torch.cat((all_probs, probs), dim=0)\n",
    "\n",
    "    return all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe341e3dbf71446fa033f9e8ab960d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1797, 10])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_train = predict(model, train_loader)\n",
    "probabilities_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, data_loader, loss_fn, device=\"cpu\"):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in tqdm(data_loader, desc=\"Scoring\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output, targets)\n",
    "            total_loss += loss.data.item() * inputs.size(0)\n",
    "\n",
    "            correct = torch.eq(torch.argmax(output, dim=1), targets)\n",
    "            total_correct += torch.sum(correct).item()\n",
    "\n",
    "    return total_loss / len(data_loader.dataset), total_correct / len(\n",
    "        data_loader.dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbc3545e734483ab2bff645e28fa5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy from score function: 1.00 and loss is 0.00\n"
     ]
    }
   ],
   "source": [
    "loss_train, accuracy_train = score(model, train_loader, criterion)\n",
    "print(f\"Training accuracy from score function: {accuracy_train:.2f} and loss is {loss_train:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Shanaly AI @ Alfred Baraka Rugoye*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
